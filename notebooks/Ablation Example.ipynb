{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0f01dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\nick\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Importing the libraries needed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import transformers\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87413014",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2b4f968",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LongformerConfig, LongformerModel, LongformerTokenizer\n",
    "tokenizer = LongformerTokenizer.from_pretrained(\"allenai/longformer-base-4096\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7caa797a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "device = 0\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23652413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining some key variables that will be used later on in the training\n",
    "MAX_LEN = 512\n",
    "\n",
    "LEARNING_RATE = 1e-05\n",
    "tokenizer = LongformerTokenizer.from_pretrained(\"allenai/longformer-base-4096\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78beb247",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/twibot20/train.json') as f:\n",
    "    train_data = json.loads(f.read())\n",
    "\n",
    "with open('../data/twibot20/val.json') as f:\n",
    "    val_data = json.loads(f.read())\n",
    "    \n",
    "with open('../data/twibot20/test.json') as f:\n",
    "    test_data = json.loads(f.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3b53aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LongformerModel(\n",
       "  (embeddings): LongformerEmbeddings(\n",
       "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(4098, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): LongformerEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): LongformerLayer(\n",
       "        (attention): LongformerAttention(\n",
       "          (self): LongformerSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (output): LongformerSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): LongformerIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): LongformerOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): LongformerLayer(\n",
       "        (attention): LongformerAttention(\n",
       "          (self): LongformerSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (output): LongformerSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): LongformerIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): LongformerOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): LongformerLayer(\n",
       "        (attention): LongformerAttention(\n",
       "          (self): LongformerSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (output): LongformerSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): LongformerIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): LongformerOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): LongformerLayer(\n",
       "        (attention): LongformerAttention(\n",
       "          (self): LongformerSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (output): LongformerSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): LongformerIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): LongformerOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): LongformerLayer(\n",
       "        (attention): LongformerAttention(\n",
       "          (self): LongformerSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (output): LongformerSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): LongformerIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): LongformerOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): LongformerLayer(\n",
       "        (attention): LongformerAttention(\n",
       "          (self): LongformerSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (output): LongformerSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): LongformerIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): LongformerOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): LongformerLayer(\n",
       "        (attention): LongformerAttention(\n",
       "          (self): LongformerSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (output): LongformerSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): LongformerIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): LongformerOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): LongformerLayer(\n",
       "        (attention): LongformerAttention(\n",
       "          (self): LongformerSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (output): LongformerSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): LongformerIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): LongformerOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): LongformerLayer(\n",
       "        (attention): LongformerAttention(\n",
       "          (self): LongformerSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (output): LongformerSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): LongformerIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): LongformerOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): LongformerLayer(\n",
       "        (attention): LongformerAttention(\n",
       "          (self): LongformerSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (output): LongformerSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): LongformerIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): LongformerOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): LongformerLayer(\n",
       "        (attention): LongformerAttention(\n",
       "          (self): LongformerSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (output): LongformerSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): LongformerIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): LongformerOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): LongformerLayer(\n",
       "        (attention): LongformerAttention(\n",
       "          (self): LongformerSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (output): LongformerSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): LongformerIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): LongformerOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): LongformerPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LongformerModel.from_pretrained(\"allenai/longformer-base-4096\")\n",
    "model.to(device)\n",
    "model.requires_grad = False\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15728910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet_data(data):\n",
    "    if len(data['tweets']) == 0:\n",
    "        text = \"\"\n",
    "    else:\n",
    "        tweet_list = np.random.choice(data['tweets'], 1000)\n",
    "        text = \" \".join(\" \".join(tweet_list).split(' ')[:MAX_LEN])\n",
    "\n",
    "    if data['label'] == 'bot':\n",
    "        label = 1\n",
    "    else:\n",
    "        label = 0\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        text,\n",
    "        max_length=MAX_LEN,\n",
    "        pad_to_max_length=True,\n",
    "        return_token_type_ids=True,\n",
    "        add_special_tokens=True\n",
    "    )\n",
    "        \n",
    "    ids = inputs['input_ids']\n",
    "    mask = inputs['attention_mask']\n",
    "    token_type_ids = inputs[\"token_type_ids\"]\n",
    "    \n",
    "    output = model(input_ids=torch.tensor(ids, dtype=torch.long).reshape(1, -1).cuda(), attention_mask=torch.tensor(mask, dtype=torch.long).reshape(1, -1).cuda(), token_type_ids=torch.tensor(token_type_ids, dtype=torch.long).reshape(1, -1).cuda())[0][0, 0, :].detach()\n",
    "    tweet_embed = output\n",
    "    \n",
    "    created_dt = pd.to_datetime(data['created_at'])\n",
    "    created_hms = np.zeros(24 + 60 + 60)\n",
    "    created_hms[created_dt.hour] = 1\n",
    "    created_hms[created_dt.minute + 24] = 1\n",
    "    created_hms[created_dt.second + 84] = 1\n",
    "    \n",
    "    \n",
    "    text = str(data['description'])\n",
    "    text = \" \".join(text.split())\n",
    "\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=MAX_LEN,\n",
    "        pad_to_max_length=True,\n",
    "        return_token_type_ids=True\n",
    "    )\n",
    "    ids = inputs['input_ids']\n",
    "    mask = inputs['attention_mask']\n",
    "    token_type_ids = inputs[\"token_type_ids\"]\n",
    "    \n",
    "    output = model(input_ids=torch.tensor(ids, dtype=torch.long).reshape(1, -1).cuda(), attention_mask=torch.tensor(mask, dtype=torch.long).reshape(1, -1).cuda(), token_type_ids=torch.tensor(token_type_ids, dtype=torch.long).reshape(1, -1).cuda())[0][0, 0, :].detach()\n",
    "    desc_embed = output\n",
    "    \n",
    "\n",
    "    return {\n",
    "        'tweet_embed': tweet_embed.detach().cpu(),\n",
    "        'desc_embed': desc_embed.detach().cpu(),\n",
    "        'hms': torch.tensor(created_hms, dtype = torch.float32),\n",
    "        'metrics': torch.tensor(list(data['public_metrics'].values()), dtype = torch.float32),\n",
    "        'verified': not 'false' in data['verified'],\n",
    "        'targets': torch.tensor(label, dtype=torch.float32)\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "124692b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserTweetDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, max_len):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.dataset[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad1e8add",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pickle.load(open(f\"../processed_data/train_set_{MAX_LEN}_long\", \"rb\"))\n",
    "val_set = pickle.load(open(f\"../processed_data/val_set_{MAX_LEN}_long\", \"rb\"))\n",
    "test_set = pickle.load(open(f\"../processed_data/test_set_{MAX_LEN}_long\", \"rb\"))\n",
    "\n",
    "train_adj = pickle.load(open(f\"../processed_data/train_adj\", \"rb\"))\n",
    "val_adj = pickle.load(open(f\"../processed_data/val_adj\", \"rb\"))\n",
    "test_adj = pickle.load(open(f\"../processed_data/test_adj\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c2a5d41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train_adj)):\n",
    "    train_set[i]['adjacency1'] = train_adj[i]['adjacency1']\n",
    "    train_set[i]['ind'] = train_adj[i]['ind']\n",
    "    \n",
    "for i in range(len(val_adj)):\n",
    "    val_set[i]['adjacency1'] = val_adj[i]['adjacency1']\n",
    "    val_set[i]['ind'] = val_adj[i]['ind']\n",
    "\n",
    "for i in range(len(test_adj)):\n",
    "    test_set[i]['adjacency1'] = test_adj[i]['adjacency1']\n",
    "    test_set[i]['ind'] = test_adj[i]['ind']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62be6e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    train_set = pickle.load(open(f\"../processed_data/train_adj\", \"rb\"))\n",
    "    val_set = pickle.load(open(f\"../processed_data/val_adj\", \"rb\"))\n",
    "    test_set = pickle.load(open(f\"../processed_data/test_adj\", \"rb\"))\n",
    "except:\n",
    "    start = time.time()\n",
    "    train_set = []\n",
    "    for i in range(len(train_data)):\n",
    "        if i % 100 == 0:\n",
    "            print(i)\n",
    "        train_set.append(process_tweet_data(train_data[i]))\n",
    "    print(time.time() - start)\n",
    "\n",
    "    start = time.time()\n",
    "    val_set = []\n",
    "    for i in range(len(val_data)):\n",
    "        val_set.append(process_tweet_data(val_data[i]))\n",
    "    print(time.time() - start)\n",
    "\n",
    "    start = time.time()\n",
    "    test_set = []\n",
    "    for i in range(len(val_data)):\n",
    "        test_set.append(process_tweet_data(test_data[i]))\n",
    "    print(time.time() - start)\n",
    "    user_data = {}\n",
    "\n",
    "    user_ids = []\n",
    "    neighbor_ids = []\n",
    "\n",
    "    for data in train_set:\n",
    "        user_ids.append(data['id'])\n",
    "        user_data[data['id']] = data\n",
    "\n",
    "    for data in val_set:\n",
    "        user_ids.append(data['id'])\n",
    "        user_data[data['id']] = data\n",
    "\n",
    "    for data in test_set:\n",
    "        user_ids.append(data['id'])\n",
    "        user_data[data['id']] = data\n",
    "\n",
    "    user_ids = set(user_ids)\n",
    "    user_ids = list(user_ids)\n",
    "\n",
    "    id_map = [{},{}]\n",
    "    for i in range(len(user_ids)):\n",
    "        id_map[0][user_ids[i]] = i\n",
    "        id_map[1][i] = user_ids[i]\n",
    "\n",
    "    d1_adjacency = torch.zeros((len(user_ids), len(user_ids)))\n",
    "    d2_adjacency = torch.zeros((len(user_ids), len(user_ids)))\n",
    "\n",
    "    i = 0\n",
    "    for uid1 in user_data:\n",
    "        if i %100 == 0:\n",
    "            print(i)\n",
    "        i+=1\n",
    "        for uid2 in user_data:\n",
    "            if uid1 == uid2:\n",
    "                continue\n",
    "            if uid2 in user_data[uid1]['friends'] or uid2 in user_data[uid1]['follows']:\n",
    "                d1_adjacency[id_map[0][uid1], id_map[0][uid2]] = 1\n",
    "                d1_adjacency[id_map[0][uid2], id_map[0][uid1]] = 1\n",
    "\n",
    "    d2_adjacency = ((d1_adjacency @ d1_adjacency) > 0).float()\n",
    "    for i in range(len(d2_adjacency)):\n",
    "        d2_adjacency[i,i] = 0\n",
    "\n",
    "    print(torch.mean(torch.sum(d1_adjacency, axis = 0)))\n",
    "    print(torch.mean(torch.sum(d2_adjacency, axis = 0)))\n",
    "\n",
    "    d2_adjacency.shape\n",
    "\n",
    "    for data in train_set:\n",
    "        del data['friends']\n",
    "        del data['follows']\n",
    "        data['adjacency1'] = d1_adjacency[id_map[0][data['id']]] / torch.sum(d1_adjacency[id_map[0][data['id']]] + .01)\n",
    "        data['adjacency2'] = d2_adjacency[id_map[0][data['id']]] / torch.sum(d1_adjacency[id_map[0][data['id']]] + .01)\n",
    "        data['ind'] = id_map[0][data['id']]\n",
    "\n",
    "    for data in val_set:\n",
    "        del data['friends']\n",
    "        del data['follows']\n",
    "        data['adjacency1'] = d1_adjacency[id_map[0][data['id']]] / torch.sum(d1_adjacency[id_map[0][data['id']]] + .01)\n",
    "        data['adjacency2'] = d2_adjacency[id_map[0][data['id']]] / torch.sum(d1_adjacency[id_map[0][data['id']]] + .01)\n",
    "        data['ind'] = id_map[0][data['id']]\n",
    "\n",
    "    for data in test_set:\n",
    "        del data['friends']\n",
    "        del data['follows']\n",
    "        data['adjacency1'] = d1_adjacency[id_map[0][data['id']]] / torch.sum(d1_adjacency[id_map[0][data['id']]] + .01)\n",
    "        data['adjacency2'] = d2_adjacency[id_map[0][data['id']]] / torch.sum(d1_adjacency[id_map[0][data['id']]] + .01)\n",
    "        data['ind'] = id_map[0][data['id']]\n",
    "\n",
    "    pickle.dump(test_set, open(\"../processed_data/test_adj\", \"wb\"))\n",
    "    pickle.dump(train_set, open(\"../processed_data/train_adj\", \"wb\"))\n",
    "    pickle.dump(val_set, open(\"../processed_data/val_adj\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fb85f15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 64\n",
    "VALID_BATCH_SIZE = 256\n",
    "\n",
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "val_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': False,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "train_loader = DataLoader(train_set, **train_params)\n",
    "val_loader = DataLoader(val_set, **val_params)\n",
    "test_loader = DataLoader(test_set, **val_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "44e61091",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TweetModel, self).__init__()\n",
    "        \n",
    "        self.l1 = torch.nn.Linear(768, 256)\n",
    "        self.l2 = torch.nn.Linear(256, 64)\n",
    "        self.l3 = torch.nn.Linear(64, 32)\n",
    "        self.l4 = torch.nn.Linear(32, 1)\n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, data):\n",
    "        tweet_embed = data['tweet_embed'].to(device)\n",
    "        x = self.activation(self.dropout(self.l1(self.dropout(tweet_embed))))\n",
    "        x = self.activation(self.dropout(self.l2(x)))\n",
    "        x = self.activation(self.dropout(self.l3(x)))\n",
    "        x = self.l4(x)\n",
    "        output = torch.squeeze(x)\n",
    "        return output\n",
    "    \n",
    "class TweetDescModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TweetDescModel, self).__init__()\n",
    "        \n",
    "        self.tweet_l1 = torch.nn.Linear(768, 128)\n",
    "        \n",
    "        self.desc_l1 = torch.nn.Linear(768, 128)\n",
    "        \n",
    "        self.fuse_l1 = torch.nn.Linear(self.tweet_l1.out_features + self.desc_l1.out_features, 128)\n",
    "        self.fuse_l2 = torch.nn.Linear(128, 64)\n",
    "        self.fuse_l3 = torch.nn.Linear(64, 32)\n",
    "        self.fuse_l4 = torch.nn.Linear(32, 1)\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, data):\n",
    "        tweet_embed = data['tweet_embed'].to(device)\n",
    "        desc_embed = data['desc_embed'].to(device)\n",
    "        tweet_x = self.activation(self.dropout(self.tweet_l1(self.dropout(tweet_embed))))\n",
    "        desc_x = self.activation(self.dropout(self.desc_l1(self.dropout(desc_embed))))\n",
    "        \n",
    "        fuse_x = torch.cat([tweet_x, desc_x], axis = 1)\n",
    "        fuse_x = self.activation(self.dropout(self.fuse_l1(fuse_x)))\n",
    "        fuse_x = self.activation(self.dropout(self.fuse_l2(fuse_x)))\n",
    "        fuse_x = self.activation(self.dropout(self.fuse_l3(fuse_x)))\n",
    "        fuse_x = self.fuse_l4(fuse_x)\n",
    "        output = torch.squeeze(fuse_x)\n",
    "        return output\n",
    "\n",
    "class FullModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FullModel, self).__init__()\n",
    "        \n",
    "        self.tweet_l1 = torch.nn.Linear(768, 128)\n",
    "        \n",
    "        self.desc_l1 = torch.nn.Linear(768, 128)\n",
    "        \n",
    "        self.hms_l1 = torch.nn.Linear(24+60+60, 16)\n",
    "        \n",
    "        self.metrics_l1 = torch.nn.Linear(4, 16)\n",
    "        \n",
    "        self.fuse_l1 = torch.nn.Linear(self.tweet_l1.out_features + self.desc_l1.out_features + self.hms_l1.out_features + self.metrics_l1.out_features + 1, 128)\n",
    "        self.fuse_l2 = torch.nn.Linear(128, 64)\n",
    "        self.fuse_l3 = torch.nn.Linear(64, 32)\n",
    "        self.fuse_l4 = torch.nn.Linear(32, 1)\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, data):\n",
    "        tweet_embed = data['tweet_embed'].to(device)\n",
    "        desc_embed = data['desc_embed'].to(device)\n",
    "        hms = data['hms'].to(device)\n",
    "        metrics = data['metrics'].to(device)\n",
    "        verified = data['verified'].to(device).reshape(-1, 1)\n",
    "        \n",
    "        tweet_x = self.activation(self.dropout(self.tweet_l1(self.dropout(tweet_embed))))\n",
    "        desc_x = self.activation(self.dropout(self.desc_l1(self.dropout(desc_embed))))\n",
    "        \n",
    "        hms_x = self.activation(self.dropout(self.hms_l1(hms)))\n",
    "        metrics_x = self.activation(self.dropout(self.metrics_l1(metrics)))\n",
    "        \n",
    "        fuse_x = torch.cat([tweet_x, desc_x, hms_x, metrics_x, verified], axis = 1)\n",
    "        fuse_x = self.activation(self.dropout(self.fuse_l1(fuse_x)))\n",
    "        fuse_x = self.activation(self.dropout(self.fuse_l2(fuse_x)))\n",
    "        fuse_x = self.activation(self.dropout(self.fuse_l3(fuse_x)))\n",
    "        fuse_x = self.fuse_l4(fuse_x)\n",
    "        output = torch.squeeze(fuse_x)\n",
    "        return output\n",
    "    \n",
    "    def embed(self, data):\n",
    "        tweet_embed = data['tweet_embed'].to(device)\n",
    "        desc_embed = data['desc_embed'].to(device)\n",
    "        hms = data['hms'].to(device)\n",
    "        metrics = data['metrics'].to(device)\n",
    "        verified = data['verified'].to(device).reshape(-1, 1)\n",
    "        \n",
    "        tweet_x = self.activation(self.dropout(self.tweet_l1(self.dropout(tweet_embed))))\n",
    "        desc_x = self.activation(self.dropout(self.desc_l1(self.dropout(desc_embed))))\n",
    "        \n",
    "        hms_x = self.activation(self.dropout(self.hms_l1(hms)))\n",
    "        metrics_x = self.activation(self.dropout(self.metrics_l1(metrics)))\n",
    "        \n",
    "        fuse_x = torch.cat([tweet_x, desc_x, hms_x, metrics_x, verified], axis = 1)\n",
    "        fuse_x = self.activation(self.dropout(self.fuse_l1(fuse_x)))\n",
    "        fuse_x = self.activation(self.dropout(self.fuse_l2(fuse_x)))\n",
    "        return fuse_x\n",
    "    \n",
    "class EarlyFullModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EarlyFullModel, self).__init__()\n",
    "\n",
    "        self.fuse_l1 = torch.nn.Linear(768 + 768 + 24 + 60 + 60 + 4 + 1, 256)\n",
    "        self.fuse_l2 = torch.nn.Linear(256, 128)\n",
    "        self.fuse_l3 = torch.nn.Linear(128, 64)\n",
    "        self.fuse_l4 = torch.nn.Linear(64, 32)\n",
    "        self.fuse_l5 = torch.nn.Linear(32, 1)\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, data):\n",
    "        tweet_embed = data['tweet_embed'].to(device)\n",
    "        desc_embed = data['desc_embed'].to(device)\n",
    "        hms = data['hms'].to(device)\n",
    "        metrics = data['metrics'].to(device)\n",
    "        verified = data['verified'].to(device).reshape(-1, 1)\n",
    "        \n",
    "        fuse_x = torch.cat([tweet_embed, desc_embed, hms, metrics, verified], axis = 1)\n",
    "        fuse_x = self.activation(self.dropout(self.fuse_l1(fuse_x)))\n",
    "        fuse_x = self.activation(self.dropout(self.fuse_l2(fuse_x)))\n",
    "        fuse_x = self.activation(self.dropout(self.fuse_l3(fuse_x)))\n",
    "        fuse_x = self.activation(self.dropout(self.fuse_l4(fuse_x)))\n",
    "        fuse_x = self.fuse_l5(fuse_x)\n",
    "        output = torch.squeeze(fuse_x)\n",
    "        return output\n",
    "    \n",
    "    def embed(self, data):\n",
    "        tweet_embed = data['tweet_embed'].to(device)\n",
    "        desc_embed = data['desc_embed'].to(device)\n",
    "        hms = data['hms'].to(device)\n",
    "        metrics = data['metrics'].to(device)\n",
    "        verified = data['verified'].to(device).reshape(-1, 1)\n",
    "        \n",
    "        fuse_x = torch.cat([tweet_embed, desc_embed, hms, metrics, verified], axis = 1)\n",
    "        fuse_x = self.activation(self.dropout(self.fuse_l1(fuse_x)))\n",
    "        fuse_x = self.activation(self.dropout(self.fuse_l2(fuse_x)))\n",
    "        fuse_x = self.fuse_l3(fuse_x)\n",
    "        return fuse_x\n",
    "    \n",
    "    \n",
    "class LateFullModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LateFullModel, self).__init__()\n",
    "        \n",
    "        self.tweet_l1 = torch.nn.Linear(768, 128)\n",
    "        self.tweet_l2 = torch.nn.Linear(128, 64)\n",
    "        self.tweet_l3 = torch.nn.Linear(64, 32)\n",
    "        \n",
    "        self.desc_l1 = torch.nn.Linear(768, 128)\n",
    "        self.desc_l2 = torch.nn.Linear(128, 64)\n",
    "        self.desc_l3 = torch.nn.Linear(64, 32)\n",
    "        \n",
    "        self.hms_l1 = torch.nn.Linear(24+60+60, 16)\n",
    "        \n",
    "        self.metrics_l1 = torch.nn.Linear(4, 16)\n",
    "        \n",
    "        self.fuse_l1 = torch.nn.Linear(self.tweet_l3.out_features + self.desc_l3.out_features + self.hms_l1.out_features + self.metrics_l1.out_features + 1, 64)\n",
    "        self.fuse_l2 = torch.nn.Linear(64, 1)\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, data):\n",
    "        tweet_embed = data['tweet_embed'].to(device)\n",
    "        desc_embed = data['desc_embed'].to(device)\n",
    "        hms = data['hms'].to(device)\n",
    "        metrics = data['metrics'].to(device)\n",
    "        verified = data['verified'].to(device).reshape(-1, 1)\n",
    "        \n",
    "        tweet_x = self.activation(self.dropout(self.tweet_l1(self.dropout(tweet_embed))))\n",
    "        tweet_x = self.activation(self.dropout(self.tweet_l2(tweet_x)))\n",
    "        tweet_x = self.activation(self.dropout(self.tweet_l3(tweet_x)))\n",
    "        \n",
    "        desc_x = self.activation(self.dropout(self.desc_l1(self.dropout(desc_embed))))\n",
    "        desc_x = self.activation(self.dropout(self.desc_l2(desc_x)))\n",
    "        desc_x = self.activation(self.dropout(self.desc_l3(desc_x)))\n",
    "        \n",
    "        hms_x = self.activation(self.dropout(self.hms_l1(self.dropout(hms))))\n",
    "        \n",
    "        metrics_x = self.activation(self.dropout(self.metrics_l1(self.dropout(metrics))))\n",
    "\n",
    "        fuse_x = torch.cat([tweet_x, desc_x, hms_x, metrics_x, verified], axis = 1)\n",
    "        fuse_x = self.activation(self.dropout(self.fuse_l1(fuse_x)))\n",
    "        fuse_x = self.fuse_l2(fuse_x)\n",
    "        output = torch.squeeze(fuse_x)\n",
    "        return output\n",
    "    \n",
    "    def embed(self, data):\n",
    "        tweet_embed = data['tweet_embed'].to(device)\n",
    "        desc_embed = data['desc_embed'].to(device)\n",
    "        hms = data['hms'].to(device)\n",
    "        metrics = data['metrics'].to(device)\n",
    "        verified = data['verified'].to(device).reshape(-1, 1)\n",
    "        \n",
    "        tweet_x = self.activation(self.dropout(self.tweet_l1(self.dropout(tweet_embed))))\n",
    "        tweet_x = self.activation(self.dropout(self.tweet_l2(tweet_x)))\n",
    "        tweet_x = self.activation(self.dropout(self.tweet_l3(tweet_x)))\n",
    "        \n",
    "        desc_x = self.activation(self.dropout(self.desc_l1(self.dropout(desc_embed))))\n",
    "        desc_x = self.activation(self.dropout(self.desc_l2(desc_x)))\n",
    "        desc_x = self.activation(self.dropout(self.desc_l3(desc_x)))\n",
    "        \n",
    "        hms_x = self.activation(self.dropout(self.hms_l1(self.dropout(hms))))\n",
    "\n",
    "        metrics_x = self.activation(self.dropout(self.metrics_l1(self.dropout(metrics))))\n",
    "        \n",
    "        fuse_x = torch.cat([tweet_x, desc_x, hms_x, metrics_x, verified], axis = 1)\n",
    "        fuse_x = self.fuse_l1(fuse_x)\n",
    "        output = torch.squeeze(fuse_x)\n",
    "        return output\n",
    "    \n",
    "class GraphModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GraphModel, self).__init__()\n",
    "        \n",
    "        self.embed_memo = torch.zeros(len(train_set[0]['adjacency1']), 64).cuda()\n",
    "        self.embed_memo.requires_grad = False\n",
    "        self.full_model = FullModel()\n",
    "        \n",
    "        self.fuse_l3 = torch.nn.Linear(128, 32)\n",
    "        self.fuse_l4 = torch.nn.Linear(32, 1)\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, data):\n",
    "        data_embed = self.full_model.embed(data)\n",
    "        #self.embed_memo[data['id']] = data_embed\n",
    "        \n",
    "        adjacency = data['adjacency1'].cuda() @ self.embed_memo\n",
    "        \n",
    "        #print(data_embed.shape, adjacency.shape)\n",
    "        \n",
    "        fuse_x = torch.cat([data_embed, adjacency], axis = 1)\n",
    "        \n",
    "        fuse_x = self.activation(self.dropout(self.fuse_l3(fuse_x)))\n",
    "        fuse_x = self.fuse_l4(fuse_x)\n",
    "        output = torch.squeeze(fuse_x)\n",
    "        return output\n",
    "    \n",
    "    def update_embed(self, data):\n",
    "        data_embed = self.full_model.embed(data)\n",
    "        self.embed_memo[data['ind']] = data_embed.detach()\n",
    "    \n",
    "    def embed(self, data):\n",
    "        return self.full_model.embed(data)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a298d5ec",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphModel(\n",
       "  (full_model): FullModel(\n",
       "    (tweet_l1): Linear(in_features=768, out_features=128, bias=True)\n",
       "    (desc_l1): Linear(in_features=768, out_features=128, bias=True)\n",
       "    (hms_l1): Linear(in_features=144, out_features=16, bias=True)\n",
       "    (metrics_l1): Linear(in_features=4, out_features=16, bias=True)\n",
       "    (fuse_l1): Linear(in_features=289, out_features=128, bias=True)\n",
       "    (fuse_l2): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (fuse_l3): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (fuse_l4): Linear(in_features=32, out_features=1, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (activation): ReLU()\n",
       "  )\n",
       "  (fuse_l3): Linear(in_features=128, out_features=32, bias=True)\n",
       "  (fuse_l4): Linear(in_features=32, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (activation): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GraphModel()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b16ac3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the loss function and optimizer\n",
    "loss_function = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=2e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "af1a644c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcuate_accuracy(preds, targets):\n",
    "    n_correct = (preds==targets).sum().item()\n",
    "    return n_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "923a18f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, dataset):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    tn = 0\n",
    "    fn = 0\n",
    "    model.eval()\n",
    "    for data in dataset:\n",
    "        preds = model(data)\n",
    "        targets = data['targets']\n",
    "        for i in range(len(preds)):\n",
    "            pred = preds[i]\n",
    "            target = targets[i]\n",
    "            if pred > 0:\n",
    "                if target == 1:\n",
    "                    tp += 1\n",
    "                else:\n",
    "                    fp += 1\n",
    "            else:\n",
    "                if target == 1:\n",
    "                    fn += 1\n",
    "                else:\n",
    "                    tn += 1\n",
    "    recall = tp / (tp + fn)\n",
    "    precision = tp / (tp + fp)\n",
    "    f1 = 2 * recall * precision / (recall + precision)\n",
    "    accuracy = (tp + tn) / (tp + tn + fn + fp)\n",
    "    return accuracy, f1, recall, precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2e7c714e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the training function on the 80% of the dataset for tuning the distilbert model\n",
    "\n",
    "def train(epoch, verbose = False, update_e = False):\n",
    "    start = time.time()\n",
    "    tr_loss = 0\n",
    "    n_correct = 0\n",
    "    nb_tr_steps = 0\n",
    "    nb_tr_examples = 0\n",
    "    model.train()\n",
    "    \n",
    "    if update_e:\n",
    "        for data in train_loader:\n",
    "            model.update_embed(data)\n",
    "\n",
    "        for data in val_loader:\n",
    "            model.update_embed(data)\n",
    "\n",
    "        for data in test_loader:\n",
    "            model.update_embed(data)\n",
    "    \n",
    "    for data in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        targets = data['targets'].to(device, dtype = torch.float32)\n",
    "\n",
    "        outputs = model(data)\n",
    "        loss = loss_function(outputs, targets)\n",
    "        tr_loss += loss.item() * targets.size(0)\n",
    "        \n",
    "        preds = (outputs.data).float() > 0\n",
    "        n_correct += calcuate_accuracy(preds, targets)\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples+=targets.size(0)\n",
    "\n",
    "        \n",
    "        loss.backward()\n",
    "        # # When using GPU\n",
    "        optimizer.step()\n",
    "        \n",
    "    model.eval()\n",
    "    \n",
    "    \n",
    "    if verbose:\n",
    "        print()\n",
    "        print(f\"Training Stats Epoch: {eval_model(model, train_loader)}\")\n",
    "        print(f\"Val Stats Epoch: {eval_model(model, val_loader)}\")\n",
    "        print(f\"Test Stats Epoch: {eval_model(model, test_loader)}\")\n",
    "        print(f\"t={time.time() - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "853a5228",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Stats Epoch: (0.6275670451799952, 0.6247108947048082, 0.5523030563925958, 0.7189688988512188)\n",
      "Val Stats Epoch: (0.6342494714587738, 0.6289146289146289, 0.5625479662317728, 0.7130350194552529)\n",
      "Test Stats Epoch: (0.6500422654268808, 0.6336283185840706, 0.559375, 0.7306122448979592)\n",
      "t=16.59629464149475\n",
      "\n",
      "Training Stats Epoch: (0.5612466779415318, 0.7189724543484989, 1.0, 0.5612466779415318)\n",
      "Val Stats Epoch: (0.5509513742071882, 0.7104689203925845, 1.0, 0.5509513742071882)\n",
      "Test Stats Epoch: (0.5409974640743872, 0.7021393307734504, 1.0, 0.5409974640743872)\n",
      "t=15.17582893371582\n",
      "\n",
      "Training Stats Epoch: (0.684223242329065, 0.7777210884353742, 0.9842875591907017, 0.6428169806016306)\n",
      "Val Stats Epoch: (0.6866807610993657, 0.7767399819222657, 0.9892555640828856, 0.6393849206349206)\n",
      "Test Stats Epoch: (0.687235841081995, 0.7738386308068459, 0.9890625, 0.6355421686746988)\n",
      "t=13.953425645828247\n",
      "\n",
      "Training Stats Epoch: (0.7666102923411452, 0.8155784650630011, 0.9195006457167456, 0.732761578044597)\n",
      "Val Stats Epoch: (0.7653276955602537, 0.8126898413769827, 0.9240214888718342, 0.7253012048192771)\n",
      "Test Stats Epoch: (0.7641589180050719, 0.8108474576271187, 0.934375, 0.7161676646706587)\n",
      "t=15.266690015792847\n",
      "\n",
      "Training Stats Epoch: (0.7956028026093259, 0.828083722820565, 0.8770985794231597, 0.7842571208622017)\n",
      "Val Stats Epoch: (0.7961945031712474, 0.8273638968481375, 0.886415963161934, 0.7756883814640698)\n",
      "Test Stats Epoch: (0.7962806424344886, 0.8252356780275562, 0.8890625, 0.7699594046008119)\n",
      "t=14.319920539855957\n",
      "\n",
      "Training Stats Epoch: (0.8034549408069582, 0.8304678545378765, 0.8577270770555316, 0.8048879014340538)\n",
      "Val Stats Epoch: (0.7940803382663848, 0.8218075375045737, 0.861857252494244, 0.7853146853146853)\n",
      "Test Stats Epoch: (0.8021978021978022, 0.8256333830104322, 0.865625, 0.7891737891737892)\n",
      "t=14.07002854347229\n",
      "\n",
      "Training Stats Epoch: (0.8096158492389466, 0.8337903395908037, 0.8508394317692639, 0.8174110835401158)\n",
      "Val Stats Epoch: (0.7949260042283298, 0.8214942951785058, 0.8564850345356869, 0.7892503536067893)\n",
      "Test Stats Epoch: (0.8081149619611158, 0.830470500373413, 0.86875, 0.7954220314735336)\n",
      "t=15.7744779586792\n",
      "\n",
      "Training Stats Epoch: (0.8137231215269388, 0.8372043918918919, 0.8534222987516142, 0.8215913800248653)\n",
      "Val Stats Epoch: (0.7991543340380549, 0.8255600440690414, 0.8626247122026094, 0.7915492957746478)\n",
      "Test Stats Epoch: (0.801352493660186, 0.824233358264772, 0.8609375, 0.7905308464849354)\n",
      "t=13.914697885513306\n",
      "\n",
      "Training Stats Epoch: (0.8158975597970525, 0.8404856604563532, 0.8641842445114076, 0.8180521597392013)\n",
      "Val Stats Epoch: (0.8025369978858351, 0.829872495446266, 0.874136607828089, 0.789875173370319)\n",
      "Test Stats Epoch: (0.7996618765849535, 0.8250922509225093, 0.8734375, 0.7818181818181819)\n",
      "t=14.35414457321167\n",
      "\n",
      "Training Stats Epoch: (0.8138439236530562, 0.8409536587883166, 0.8768833405079638, 0.8078524687685901)\n",
      "Val Stats Epoch: (0.8, 0.8300395256916996, 0.886415963161934, 0.7804054054054054)\n",
      "Test Stats Epoch: (0.8005071851225697, 0.8267254038179148, 0.8796875, 0.7797783933518005)\n",
      "t=15.030683994293213\n",
      "\n",
      "Training Stats Epoch: (0.8277361681565596, 0.8505554391112975, 0.87343951786483, 0.8288398692810458)\n",
      "Val Stats Epoch: (0.7991543340380549, 0.827335514358415, 0.8733691481197237, 0.7859116022099447)\n",
      "Test Stats Epoch: (0.7988165680473372, 0.8239644970414202, 0.8703125, 0.7823033707865169)\n",
      "t=14.508337020874023\n",
      "\n",
      "Training Stats Epoch: (0.830152210678908, 0.8535721724640699, 0.8820490744726647, 0.8268765133171913)\n",
      "Val Stats Epoch: (0.8016913319238901, 0.8298875589408778, 0.8779739063699156, 0.7867950481430537)\n",
      "Test Stats Epoch: (0.8005071851225697, 0.8249258160237389, 0.86875, 0.7853107344632768)\n",
      "t=15.012773036956787\n",
      "\n",
      "Training Stats Epoch: (0.8365547233631312, 0.8590478174809877, 0.8874300473525614, 0.8324247930547143)\n",
      "Val Stats Epoch: (0.8038054968287527, 0.8302852962692027, 0.8710667689946278, 0.793151642208246)\n",
      "Test Stats Epoch: (0.7988165680473372, 0.8226527570789866, 0.8625, 0.7863247863247863)\n",
      "t=14.869764804840088\n",
      "\n",
      "Training Stats Epoch: (0.84223242329065, 0.8649431230610135, 0.9001291433491175, 0.8324044585987261)\n",
      "Val Stats Epoch: (0.8054968287526427, 0.8328488372093023, 0.8795088257866462, 0.7908902691511387)\n",
      "Test Stats Epoch: (0.8047337278106509, 0.8279970215934476, 0.86875, 0.7908961593172119)\n",
      "t=16.013516664505005\n",
      "\n",
      "Training Stats Epoch: (0.8462188934525248, 0.8663517060367454, 0.8880757640981489, 0.8456650953064152)\n",
      "Val Stats Epoch: (0.8093023255813954, 0.8331483536810951, 0.86415963161934, 0.8042857142857143)\n",
      "Test Stats Epoch: (0.8182586644125106, 0.8360030511060258, 0.85625, 0.8166915052160953)\n",
      "t=14.258457899093628\n",
      "\n",
      "Training Stats Epoch: (0.8521381976322784, 0.8740481580572135, 0.9141196728368489, 0.8373422712933754)\n",
      "Val Stats Epoch: (0.8021141649048625, 0.8280675973548862, 0.8649270913277053, 0.7942212825933757)\n",
      "Test Stats Epoch: (0.8021978021978022, 0.8245877061469267, 0.859375, 0.792507204610951)\n",
      "t=15.219509840011597\n",
      "\n",
      "Training Stats Epoch: (0.8610775549649674, 0.881100082712986, 0.9171330176495911, 0.847791484281735)\n",
      "Val Stats Epoch: (0.7966173361522199, 0.82348623853211, 0.8610897927858787, 0.7890295358649789)\n",
      "Test Stats Epoch: (0.8047337278106509, 0.8261851015801355, 0.8578125, 0.7968069666182874)\n",
      "t=13.984068632125854\n",
      "\n",
      "Training Stats Epoch: (0.863251993235081, 0.8827914682128805, 0.9175634954799827, 0.8505586592178771)\n",
      "Val Stats Epoch: (0.8021141649048625, 0.8279411764705883, 0.86415963161934, 0.7946365561044461)\n",
      "Test Stats Epoch: (0.8021978021978022, 0.823262839879154, 0.8515625, 0.7967836257309941)\n",
      "t=14.016974925994873\n",
      "\n",
      "Training Stats Epoch: (0.8775066441169365, 0.8930605357519511, 0.9113215669393027, 0.8755169561621174)\n",
      "Val Stats Epoch: (0.7983086680761099, 0.8208787082238078, 0.8388334612432847, 0.8036764705882353)\n",
      "Test Stats Epoch: (0.801352493660186, 0.8182521268368137, 0.8265625, 0.8101071975497703)\n",
      "t=15.47493600845337\n",
      "\n",
      "Training Stats Epoch: (0.8806474993959894, 0.8971047698396168, 0.9270340077486009, 0.8690476190476191)\n",
      "Val Stats Epoch: (0.8016913319238901, 0.8257153474544778, 0.8526477359938603, 0.8004322766570605)\n",
      "Test Stats Epoch: (0.801352493660186, 0.8226415094339622, 0.8515625, 0.7956204379562044)\n",
      "t=15.414870977401733\n",
      "\n",
      "Training Stats Epoch: (0.8848755738100991, 0.9013967925504397, 0.9375807145931985, 0.867901972504483)\n",
      "Val Stats Epoch: (0.7966173361522199, 0.8230967267377713, 0.8587874136607828, 0.7902542372881356)\n",
      "Test Stats Epoch: (0.801352493660186, 0.8231753197893152, 0.8546875, 0.7939042089985486)\n",
      "t=14.331090211868286\n",
      "\n",
      "Training Stats Epoch: (0.8936941290166707, 0.9069373942470389, 0.9229444683598794, 0.8914760914760915)\n",
      "Val Stats Epoch: (0.7940803382663848, 0.8164342254052016, 0.8311588641596316, 0.8022222222222222)\n",
      "Test Stats Epoch: (0.7954353338968724, 0.8149847094801222, 0.8328125, 0.7979041916167665)\n",
      "t=15.448629140853882\n",
      "\n",
      "Training Stats Epoch: (0.8945397438994926, 0.9093552071435987, 0.9425312096427034, 0.8784353059177532)\n",
      "Val Stats Epoch: (0.7995771670190275, 0.8265007320644217, 0.866462010744436, 0.7900629811056683)\n",
      "Test Stats Epoch: (0.7971259509721048, 0.8198198198198199, 0.853125, 0.7890173410404624)\n",
      "t=14.113804817199707\n",
      "\n",
      "Training Stats Epoch: (0.8999758395747766, 0.9132257388388177, 0.9377959535083943, 0.8899101307189542)\n",
      "Val Stats Epoch: (0.7978858350951374, 0.823876197494473, 0.8580199539524175, 0.7923458540042523)\n",
      "Test Stats Epoch: (0.7945900253592562, 0.8166037735849057, 0.8453125, 0.7897810218978102)\n",
      "t=15.358707427978516\n",
      "\n",
      "Training Stats Epoch: (0.9035999033582991, 0.9163171140939598, 0.9403788204907447, 0.8934560327198364)\n",
      "Val Stats Epoch: (0.7974630021141649, 0.8238322912835603, 0.8595548733691482, 0.7909604519774012)\n",
      "Test Stats Epoch: (0.797971259509721, 0.8207051762940736, 0.8546875, 0.7893217893217893)\n",
      "t=15.460000276565552\n",
      "\n",
      "Training Stats Epoch: (0.9126600628171055, 0.9234839665573077, 0.9390873869995695, 0.9083905892150739)\n",
      "Val Stats Epoch: (0.7995771670190275, 0.8235294117647058, 0.8488104374520338, 0.7997107736804049)\n",
      "Test Stats Epoch: (0.7988165680473372, 0.8188736681887367, 0.840625, 0.798219584569733)\n",
      "t=13.99757432937622\n",
      "\n",
      "Training Stats Epoch: (0.9137472819521624, 0.9247311827956989, 0.9440378820490745, 0.906198347107438)\n",
      "Val Stats Epoch: (0.7945031712473573, 0.8219780219780219, 0.8610897927858787, 0.7862648913805186)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Stats Epoch: (0.7996618765849535, 0.8216704288939052, 0.853125, 0.7924528301886793)\n",
      "t=14.161805391311646\n",
      "\n",
      "Training Stats Epoch: (0.9170089393573327, 0.9269537480063795, 0.938226431338786, 0.9159487287245219)\n",
      "Val Stats Epoch: (0.7932346723044398, 0.8189559422436135, 0.8488104374520338, 0.7911301859799714)\n",
      "Test Stats Epoch: (0.7920540997464074, 0.8127853881278538, 0.834375, 0.7922848664688428)\n",
      "t=15.558864116668701\n",
      "\n",
      "Training Stats Epoch: (0.9176129499879198, 0.9276776246023329, 0.941455015066724, 0.9142976588628763)\n",
      "Val Stats Epoch: (0.7978858350951374, 0.8233555062823354, 0.8549501151189562, 0.7940128296507484)\n",
      "Test Stats Epoch: (0.7928994082840237, 0.8139711465451784, 0.8375, 0.7917282127031019)\n",
      "t=13.993781328201294\n",
      "\n",
      "Training Stats Epoch: (0.9238946605460256, 0.9337121212121212, 0.9550150667240637, 0.9133388225607246)\n",
      "Val Stats Epoch: (0.7966173361522199, 0.8238740388136213, 0.8633921719109747, 0.7878151260504201)\n",
      "Test Stats Epoch: (0.7895181741335587, 0.8134831460674158, 0.8484375, 0.781294964028777)\n",
      "t=15.485004663467407\n",
      "\n",
      "Training Stats Epoch: (0.9313843923653057, 0.9395358739621035, 0.9498493327593629, 0.9294439764111204)\n",
      "Val Stats Epoch: (0.7898520084566596, 0.8153102935711631, 0.841903300076746, 0.7903458213256485)\n",
      "Test Stats Epoch: (0.7971259509721048, 0.816793893129771, 0.8359375, 0.7985074626865671)\n",
      "t=15.256220817565918\n",
      "\n",
      "Training Stats Epoch: (0.9310219859869534, 0.9390282968499732, 0.946405510116229, 0.9317652044924772)\n",
      "Val Stats Epoch: (0.7915433403805496, 0.8190825688073394, 0.8564850345356869, 0.7848101265822784)\n",
      "Test Stats Epoch: (0.790363482671175, 0.8129713423831071, 0.8421875, 0.7857142857142857)\n",
      "t=14.082716464996338\n",
      "\n",
      "Training Stats Epoch: (0.9325924136264798, 0.9406256650351138, 0.9513560051657339, 0.9301346801346801)\n",
      "Val Stats Epoch: (0.7966173361522199, 0.8240029271862422, 0.86415963161934, 0.7874125874125875)\n",
      "Test Stats Epoch: (0.7928994082840237, 0.8153730218538056, 0.8453125, 0.7874818049490538)\n",
      "t=15.320523977279663\n",
      "\n",
      "Training Stats Epoch: (0.9328340178787147, 0.9402407566638005, 0.941455015066724, 0.9390296264491198)\n",
      "Val Stats Epoch: (0.7995771670190275, 0.8237918215613381, 0.8503453568687643, 0.798846431146359)\n",
      "Test Stats Epoch: (0.7861369399830939, 0.8055342044581092, 0.81875, 0.7927382753403933)\n",
      "t=15.408235788345337\n",
      "\n",
      "Training Stats Epoch: (0.9388741241845856, 0.945847602739726, 0.9511407662505381, 0.9406130268199234)\n",
      "Val Stats Epoch: (0.7983086680761099, 0.8245678558293489, 0.8603223330775134, 0.7916666666666666)\n",
      "Test Stats Epoch: (0.7912087912087912, 0.8141459744168548, 0.8453125, 0.7851959361393324)\n",
      "t=13.809413194656372\n",
      "\n",
      "Training Stats Epoch: (0.9346460497704759, 0.9425750981849061, 0.9556607834696513, 0.9298429319371728)\n",
      "Val Stats Epoch: (0.7915433403805496, 0.82040072859745, 0.86415963161934, 0.7808599167822469)\n",
      "Test Stats Epoch: (0.7962806424344886, 0.8189331329827197, 0.8515625, 0.788712011577424)\n",
      "t=13.876949787139893\n",
      "\n",
      "Training Stats Epoch: (0.9369412901667069, 0.9441591784338896, 0.9498493327593629, 0.9385367928541046)\n",
      "Val Stats Epoch: (0.7966173361522199, 0.8236156949028236, 0.861857252494244, 0.788623595505618)\n",
      "Test Stats Epoch: (0.8005071851225697, 0.8217522658610271, 0.85, 0.7953216374269005)\n",
      "t=15.294863939285278\n",
      "\n",
      "Training Stats Epoch: (0.9440686156076347, 0.950380452255921, 0.9543693499784761, 0.9464247598719316)\n",
      "Val Stats Epoch: (0.7966173361522199, 0.8241316270566728, 0.8649270913277053, 0.7870111731843575)\n",
      "Test Stats Epoch: (0.7869822485207101, 0.8096676737160121, 0.8375, 0.783625730994152)\n",
      "t=15.331597805023193\n",
      "\n",
      "Training Stats Epoch: (0.9493839091568012, 0.9550862900632435, 0.9588893671975893, 0.9513132607303011)\n",
      "Val Stats Epoch: (0.7966173361522199, 0.824260138838144, 0.8656945510360706, 0.7866108786610879)\n",
      "Test Stats Epoch: (0.7869822485207101, 0.8105263157894737, 0.8421875, 0.7811594202898551)\n",
      "t=13.95980167388916\n",
      "\n",
      "Training Stats Epoch: (0.9474510751389225, 0.9535107406219941, 0.9601808006887645, 0.9469327106771386)\n",
      "Val Stats Epoch: (0.7885835095137421, 0.8184458968772694, 0.8649270913277053, 0.7767057201929704)\n",
      "Test Stats Epoch: (0.7827557058326289, 0.8066215199398042, 0.8375, 0.7779390420899854)\n",
      "t=13.896992206573486\n",
      "\n",
      "Training Stats Epoch: (0.9497463155351534, 0.9551434116885918, 0.9532931554024968, 0.9570008643042351)\n",
      "Val Stats Epoch: (0.7945031712473573, 0.8207964601769913, 0.8541826554105909, 0.7899219304471257)\n",
      "Test Stats Epoch: (0.790363482671175, 0.8112633181126331, 0.8328125, 0.7908011869436202)\n",
      "t=14.115744590759277\n",
      "\n",
      "Training Stats Epoch: (0.9521623580575018, 0.9575380656229896, 0.961041756349548, 0.9540598290598291)\n",
      "Val Stats Epoch: (0.7945031712473573, 0.8228862973760933, 0.866462010744436, 0.7834836918806385)\n",
      "Test Stats Epoch: (0.7878275570583263, 0.8114199849737039, 0.84375, 0.7814761215629522)\n",
      "t=15.847288608551025\n",
      "\n",
      "Training Stats Epoch: (0.9546992027059676, 0.9599487343800064, 0.9672836848902282, 0.9527241891032436)\n",
      "Val Stats Epoch: (0.7890063424947146, 0.8183472879504914, 0.8626247122026094, 0.778393351800554)\n",
      "Test Stats Epoch: (0.7844463229078613, 0.8098434004474274, 0.8484375, 0.7746077032810271)\n",
      "t=15.443516969680786\n",
      "\n",
      "Training Stats Epoch: (0.9559072239671418, 0.9606638646405863, 0.9593198450279811, 0.9620116555147853)\n",
      "Val Stats Epoch: (0.7885835095137421, 0.8144023756495916, 0.841903300076746, 0.7886412652767792)\n",
      "Test Stats Epoch: (0.7827557058326289, 0.8033664881407804, 0.8203125, 0.7871064467766117)\n",
      "t=13.860569477081299\n",
      "\n",
      "Training Stats Epoch: (0.957115245228316, 0.9619221280703636, 0.9651312957382695, 0.9587342313448792)\n",
      "Val Stats Epoch: (0.7932346723044398, 0.8215979569500181, 0.86415963161934, 0.7830319888734353)\n",
      "Test Stats Epoch: (0.779374471682164, 0.8036117381489843, 0.834375, 0.7750362844702468)\n",
      "t=15.371850490570068\n",
      "\n",
      "Training Stats Epoch: (0.9601352983812516, 0.9644702842377262, 0.9640551011622901, 0.9648858250753986)\n",
      "Val Stats Epoch: (0.7940803382663848, 0.8211531399192067, 0.8580199539524175, 0.7873239436619718)\n",
      "Test Stats Epoch: (0.7836010143702451, 0.8066465256797583, 0.834375, 0.7807017543859649)\n",
      "t=15.295018196105957\n",
      "\n",
      "Training Stats Epoch: (0.9642425706692438, 0.9680965725371847, 0.9666379681446405, 0.969559585492228)\n",
      "Val Stats Epoch: (0.7822410147991543, 0.8089053803339518, 0.8365310821181888, 0.7830459770114943)\n",
      "Test Stats Epoch: (0.779374471682164, 0.8012185833968013, 0.821875, 0.7815750371471025)\n",
      "t=13.930593729019165\n",
      "\n",
      "Training Stats Epoch: (0.9621889345252477, 0.9664702731655063, 0.9709427464485579, 0.9620388142461079)\n",
      "Val Stats Epoch: (0.7890063424947146, 0.819268381021369, 0.8679969301611665, 0.7757201646090535)\n",
      "Test Stats Epoch: (0.7827557058326289, 0.8097705403404886, 0.8546875, 0.7693389592123769)\n",
      "t=15.402504444122314\n",
      "\n",
      "Training Stats Epoch: (0.9620681323991302, 0.9660980349816455, 0.9629789065863108, 0.9692374350086655)\n",
      "Val Stats Epoch: (0.7945031712473573, 0.8205317577548006, 0.8526477359938603, 0.7907473309608541)\n",
      "Test Stats Epoch: (0.7844463229078613, 0.8072562358276644, 0.834375, 0.7818448023426061)\n",
      "t=13.996400117874146\n",
      "\n",
      "Training Stats Epoch: (0.9589272771200773, 0.9630354424874973, 0.9532931554024968, 0.9729789103690686)\n",
      "Val Stats Epoch: (0.7936575052854122, 0.8177744585511577, 0.8403683806600154, 0.7963636363636364)\n",
      "Test Stats Epoch: (0.779374471682164, 0.7984555984555984, 0.8078125, 0.7893129770992366)\n",
      "t=15.408268213272095\n",
      "\n",
      "Training Stats Epoch: (0.961705726020778, 0.9660563229467823, 0.9709427464485579, 0.961218836565097)\n",
      "Val Stats Epoch: (0.7898520084566596, 0.8210298883687434, 0.8749040675364543, 0.7734056987788331)\n",
      "Test Stats Epoch: (0.7776838546069316, 0.8041697691734923, 0.84375, 0.7681365576102418)\n",
      "t=13.982427597045898\n",
      "\n",
      "Training Stats Epoch: (0.9660546025610051, 0.9696511502322065, 0.9662074903142488, 0.9731194450466074)\n",
      "Val Stats Epoch: (0.7894291754756871, 0.8161004431314623, 0.8480429777436684, 0.7864768683274022)\n",
      "Test Stats Epoch: (0.7819103972950127, 0.8057228915662652, 0.8359375, 0.7776162790697675)\n",
      "t=15.369028568267822\n",
      "\n",
      "Training Stats Epoch: (0.9704034791012321, 0.9735392590992548, 0.9700817907877745, 0.977021461088229)\n",
      "Val Stats Epoch: (0.7885835095137421, 0.8154981549815498, 0.8480429777436684, 0.7853589196872779)\n",
      "Test Stats Epoch: (0.7878275570583263, 0.8097043214556482, 0.834375, 0.7864506627393225)\n",
      "t=13.797674894332886\n",
      "\n",
      "Training Stats Epoch: (0.9659338004348876, 0.9696447793326156, 0.9694360740421868, 0.9698535745047373)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Stats Epoch: (0.786046511627907, 0.8142437591776799, 0.8511128165771297, 0.7804363124560169)\n",
      "Test Stats Epoch: (0.7751479289940828, 0.799396681749623, 0.828125, 0.7725947521865889)\n",
      "t=14.029000759124756\n",
      "\n",
      "Training Stats Epoch: (0.9695578642184103, 0.9727802981205443, 0.969220835126991, 0.9763660017346054)\n",
      "Val Stats Epoch: (0.7906976744186046, 0.8171407462135204, 0.8488104374520338, 0.7877492877492878)\n",
      "Test Stats Epoch: (0.7810650887573964, 0.8030418250950571, 0.825, 0.7822222222222223)\n",
      "t=15.460454940795898\n",
      "\n",
      "Training Stats Epoch: (0.9666586131915922, 0.9700130378096479, 0.9608265174343521, 0.9793769197016235)\n",
      "Val Stats Epoch: (0.7966173361522199, 0.8215213358070501, 0.8495778971603991, 0.7952586206896551)\n",
      "Test Stats Epoch: (0.7810650887573964, 0.8030418250950571, 0.825, 0.7822222222222223)\n",
      "t=15.164224863052368\n",
      "\n",
      "Training Stats Epoch: (0.971490698236289, 0.9745744451626804, 0.9735256134309083, 0.975625539257981)\n",
      "Val Stats Epoch: (0.7873150105708245, 0.8158183815452216, 0.8549501151189562, 0.7801120448179272)\n",
      "Test Stats Epoch: (0.7709213863060017, 0.7966991747936985, 0.8296875, 0.7662337662337663)\n",
      "t=13.79435420036316\n",
      "\n",
      "Training Stats Epoch: (0.971490698236289, 0.9744478129060198, 0.9685751183814033, 0.9803921568627451)\n",
      "Val Stats Epoch: (0.7945031712473573, 0.8203991130820399, 0.851880276285495, 0.7911617961511048)\n",
      "Test Stats Epoch: (0.7810650887573964, 0.8039364118092354, 0.8296875, 0.7797356828193832)\n",
      "t=15.195539236068726\n",
      "\n",
      "Training Stats Epoch: (0.9740275428847548, 0.9767843645394665, 0.9735256134309083, 0.980065005417118)\n",
      "Val Stats Epoch: (0.7923890063424948, 0.8199486615328201, 0.8580199539524175, 0.7851123595505618)\n",
      "Test Stats Epoch: (0.7768385460693153, 0.8009049773755657, 0.8296875, 0.7740524781341108)\n",
      "t=13.78371262550354\n",
      "\n",
      "Training Stats Epoch: (0.9778932109205122, 0.9803119956966111, 0.9806284976323719, 0.9799956979995698)\n",
      "Val Stats Epoch: (0.7835095137420719, 0.8112094395280235, 0.8442056792018419, 0.7806955287437899)\n",
      "Test Stats Epoch: (0.7734573119188504, 0.7984962406015037, 0.8296875, 0.7695652173913043)\n",
      "t=15.289106845855713\n",
      "\n",
      "Training Stats Epoch: (0.9764435854071032, 0.9789257538095754, 0.9748170469220835, 0.9830692424571305)\n",
      "Val Stats Epoch: (0.7898520084566596, 0.8165374677002584, 0.8488104374520338, 0.786628733997155)\n",
      "Test Stats Epoch: (0.7734573119188504, 0.797583081570997, 0.825, 0.7719298245614035)\n",
      "t=15.334147214889526\n",
      "\n",
      "Training Stats Epoch: (0.9751147620198115, 0.9778112882378285, 0.9769694360740422, 0.9786545924967659)\n",
      "Val Stats Epoch: (0.7915433403805496, 0.8206620589305202, 0.8656945510360706, 0.7800829875518672)\n",
      "Test Stats Epoch: (0.7852916314454776, 0.811292719167905, 0.853125, 0.773371104815864)\n",
      "t=13.942237377166748\n",
      "\n",
      "Training Stats Epoch: (0.9760811790287509, 0.9786591937917654, 0.977184674989238, 0.9801381692573402)\n",
      "Val Stats Epoch: (0.7843551797040169, 0.8129126925898753, 0.8503453568687643, 0.7786366830639494)\n",
      "Test Stats Epoch: (0.7734573119188504, 0.8002980625931445, 0.8390625, 0.7649572649572649)\n",
      "t=14.669513463973999\n",
      "\n",
      "Training Stats Epoch: (0.9786180236772167, 0.9809369951534734, 0.9801980198019802, 0.9816770855787885)\n",
      "Val Stats Epoch: (0.7864693446088795, 0.8123374210330733, 0.8388334612432847, 0.787463976945245)\n",
      "Test Stats Epoch: (0.7785291631445478, 0.802710843373494, 0.8328125, 0.7747093023255814)\n",
      "t=15.587065696716309\n",
      "\n",
      "Training Stats Epoch: (0.9774100024160425, 0.9797597142547895, 0.9741713301764959, 0.9854125843675158)\n",
      "Val Stats Epoch: (0.7826638477801269, 0.8100517368810052, 0.8411358403683806, 0.7811831789023521)\n",
      "Test Stats Epoch: (0.775993237531699, 0.8017950635751683, 0.8375, 0.7690100430416069)\n",
      "t=15.350372314453125\n",
      "\n",
      "Training Stats Epoch: (0.9788596279294516, 0.9810708491076257, 0.9761084804132587, 0.9860839312894107)\n",
      "Val Stats Epoch: (0.7894291754756871, 0.8154188287620459, 0.8442056792018419, 0.7885304659498208)\n",
      "Test Stats Epoch: (0.7810650887573964, 0.8054094665664913, 0.8375, 0.7756874095513748)\n",
      "t=13.863018035888672\n",
      "\n",
      "Training Stats Epoch: (0.9760811790287509, 0.9785389117710818, 0.9715884631941455, 0.985589519650655)\n",
      "Val Stats Epoch: (0.7885835095137421, 0.8145400593471811, 0.8426707597851113, 0.7882268485283561)\n",
      "Test Stats Epoch: (0.7810650887573964, 0.8033409263477601, 0.8265625, 0.7813884785819794)\n",
      "t=14.113415241241455\n",
      "\n",
      "Training Stats Epoch: (0.9810340661995651, 0.9831091984938138, 0.9834266035299182, 0.9827919982791998)\n",
      "Val Stats Epoch: (0.7792811839323467, 0.8093498904309715, 0.8503453568687643, 0.7721254355400697)\n",
      "Test Stats Epoch: (0.7700760777683855, 0.797317436661699, 0.8359375, 0.7621082621082621)\n",
      "t=15.276154041290283\n",
      "\n",
      "Training Stats Epoch: (0.9797052428122734, 0.981810307492421, 0.9758932414980629, 0.9877995642701525)\n",
      "Val Stats Epoch: (0.7797040169133193, 0.8050879161990272, 0.8257866462010744, 0.7854014598540145)\n",
      "Test Stats Epoch: (0.7743026204564666, 0.7954022988505747, 0.8109375, 0.7804511278195488)\n",
      "t=13.705442428588867\n",
      "\n",
      "Training Stats Epoch: (0.9776516066682773, 0.9800453025563586, 0.9778303917348257, 0.9822702702702703)\n",
      "Val Stats Epoch: (0.7932346723044398, 0.8204186558942342, 0.8572524942440521, 0.7866197183098591)\n",
      "Test Stats Epoch: (0.7819103972950127, 0.8063063063063064, 0.8390625, 0.7760115606936416)\n",
      "t=15.300627946853638\n",
      "\n",
      "Training Stats Epoch: (0.982725295965209, 0.9846087611667205, 0.9845027981058976, 0.9847147470398278)\n",
      "Val Stats Epoch: (0.7767441860465116, 0.8061674008810573, 0.8426707597851113, 0.772695285010556)\n",
      "Test Stats Epoch: (0.7692307692307693, 0.7948910593538692, 0.8265625, 0.7655571635311144)\n",
      "t=15.273545026779175\n",
      "\n",
      "Training Stats Epoch: (0.9807924619473303, 0.9827679635851305, 0.9758932414980629, 0.9897402313905261)\n",
      "Val Stats Epoch: (0.7923890063424948, 0.8168593808280493, 0.8403683806600154, 0.7946298984034833)\n",
      "Test Stats Epoch: (0.7810650887573964, 0.8024408848207475, 0.821875, 0.7839046199701938)\n",
      "t=13.735896825790405\n",
      "\n",
      "Training Stats Epoch: (0.9821212853346218, 0.984010371650821, 0.9801980198019802, 0.9878524945770065)\n",
      "Val Stats Epoch: (0.786046511627907, 0.8134218289085546, 0.8465080583269379, 0.7828246983676366)\n",
      "Test Stats Epoch: (0.7734573119188504, 0.7981927710843374, 0.828125, 0.7703488372093024)\n",
      "t=15.495412588119507\n",
      "\n",
      "Training Stats Epoch: (0.978980430055569, 0.9811197916666666, 0.9730951356005165, 0.9892778993435448)\n",
      "Val Stats Epoch: (0.7864693446088795, 0.8107905582615211, 0.8303914044512664, 0.7920937042459737)\n",
      "Test Stats Epoch: (0.775993237531699, 0.7969348659003832, 0.8125, 0.7819548872180451)\n",
      "t=15.350383996963501\n",
      "\n",
      "Training Stats Epoch: (0.9826044938390915, 0.9844357976653696, 0.9801980198019802, 0.9887103777681285)\n",
      "Val Stats Epoch: (0.7839323467230443, 0.8095415579575103, 0.8334612432847276, 0.7869565217391304)\n",
      "Test Stats Epoch: (0.7700760777683855, 0.7929984779299848, 0.8140625, 0.7729970326409495)\n",
      "t=13.750083684921265\n",
      "\n",
      "Training Stats Epoch: (0.9856245469920271, 0.9871919061457324, 0.9870856650882479, 0.9872981700753498)\n",
      "Val Stats Epoch: (0.7835095137420719, 0.8119030124908155, 0.8480429777436684, 0.7787174066243834)\n",
      "Test Stats Epoch: (0.7666948436179205, 0.7937219730941704, 0.8296875, 0.7607449856733525)\n",
      "t=13.714112281799316\n",
      "\n",
      "Training Stats Epoch: (0.986470161874849, 0.987918015102481, 0.9855789926818769, 0.9902681660899654)\n",
      "Val Stats Epoch: (0.7780126849894292, 0.8053392658509455, 0.8334612432847276, 0.7790530846484935)\n",
      "Test Stats Epoch: (0.7692307692307693, 0.7936507936507936, 0.8203125, 0.7686676427525623)\n",
      "t=15.352456331253052\n",
      "\n",
      "Training Stats Epoch: (0.986470161874849, 0.9879492145470197, 0.9881618596642273, 0.987736660929432)\n",
      "Val Stats Epoch: (0.7784355179704017, 0.8090379008746356, 0.851880276285495, 0.7702984038861902)\n",
      "Test Stats Epoch: (0.7734573119188504, 0.8023598820058997, 0.85, 0.7597765363128491)\n",
      "t=9.960452795028687\n",
      "\n",
      "Training Stats Epoch: (0.9755979705242812, 0.9780387040661014, 0.9681446405510116, 0.9881370826010545)\n",
      "Val Stats Epoch: (0.7902748414376322, 0.8165680473372782, 0.8472755180353031, 0.7880085653104925)\n",
      "Test Stats Epoch: (0.7852916314454776, 0.8072837632776936, 0.83125, 0.7846607669616519)\n",
      "t=10.202147960662842\n",
      "\n",
      "Training Stats Epoch: (0.9829669002174438, 0.9847616989084621, 0.9806284976323719, 0.988929889298893)\n",
      "Val Stats Epoch: (0.7771670190274842, 0.8038704875325643, 0.8288564850345357, 0.7803468208092486)\n",
      "Test Stats Epoch: (0.7743026204564666, 0.7969581749049429, 0.81875, 0.7762962962962963)\n",
      "t=14.019062757492065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Stats Epoch: (0.9834501087219135, 0.9851811790156842, 0.9801980198019802, 0.9902152641878669)\n",
      "Val Stats Epoch: (0.7915433403805496, 0.8188166115398752, 0.8549501151189562, 0.7856135401974612)\n",
      "Test Stats Epoch: (0.7836010143702451, 0.8083832335329341, 0.84375, 0.7758620689655172)\n",
      "t=10.220168590545654\n",
      "\n",
      "Training Stats Epoch: (0.9845373278569702, 0.9861741196802765, 0.9825656478691347, 0.9898091934084996)\n",
      "Val Stats Epoch: (0.7915433403805496, 0.8180140273163529, 0.8503453568687643, 0.7880512091038406)\n",
      "Test Stats Epoch: (0.7692307692307693, 0.7936507936507936, 0.8203125, 0.7686676427525623)\n",
      "t=13.809845685958862\n",
      "\n",
      "Training Stats Epoch: (0.9817588789562697, 0.9836562398527979, 0.9780456306500215, 0.9893315915523623)\n",
      "Val Stats Epoch: (0.7940803382663848, 0.821021683204704, 0.8572524942440521, 0.7877291960507757)\n",
      "Test Stats Epoch: (0.7844463229078613, 0.8098434004474274, 0.8484375, 0.7746077032810271)\n",
      "t=10.022267818450928\n",
      "\n",
      "Training Stats Epoch: (0.9869533703793186, 0.9883495145631067, 0.9860094705122686, 0.9907006920415224)\n",
      "Val Stats Epoch: (0.7923890063424948, 0.8211293260473588, 0.8649270913277053, 0.7815533980582524)\n",
      "Test Stats Epoch: (0.7709213863060017, 0.7991104521868051, 0.8421875, 0.7602256699576869)\n",
      "t=10.060129404067993\n",
      "\n",
      "Training Stats Epoch: (0.9850205363614399, 0.9866264020707506, 0.9845027981058976, 0.988759187202767)\n",
      "Val Stats Epoch: (0.7906976744186046, 0.8168701442841289, 0.8472755180353031, 0.7885714285714286)\n",
      "Test Stats Epoch: (0.7641589180050719, 0.7884761182714178, 0.8125, 0.7658321060382917)\n",
      "t=13.840699195861816\n",
      "\n",
      "Training Stats Epoch: (0.9886446001449626, 0.989879414298019, 0.9894532931554025, 0.9903059026281775)\n",
      "Val Stats Epoch: (0.7847780126849895, 0.814029959810011, 0.8549501151189562, 0.7768479776847977)\n",
      "Test Stats Epoch: (0.7717666948436179, 0.7982062780269058, 0.834375, 0.7650429799426934)\n",
      "t=10.166172981262207\n",
      "\n",
      "Training Stats Epoch: (0.9894902150277844, 0.990613874204337, 0.9881618596642273, 0.9930780878217608)\n",
      "Val Stats Epoch: (0.7797040169133193, 0.8053791557713859, 0.827321565617805, 0.784570596797671)\n",
      "Test Stats Epoch: (0.768385460693153, 0.790519877675841, 0.8078125, 0.7739520958083832)\n",
      "t=13.861993551254272\n",
      "\n",
      "Training Stats Epoch: (0.9881613916404929, 0.9894054054054054, 0.9849332759362893, 0.9939183318853171)\n",
      "Val Stats Epoch: (0.786046511627907, 0.8127313101406366, 0.8426707597851113, 0.7848463187991422)\n",
      "Test Stats Epoch: (0.7709213863060017, 0.7945413191811979, 0.81875, 0.7717231222385862)\n",
      "t=13.795584201812744\n",
      "\n",
      "Training Stats Epoch: (0.9890070065233149, 0.9901971345470214, 0.9892380542402066, 0.9911580763424628)\n",
      "Val Stats Epoch: (0.7818181818181819, 0.8098747236551216, 0.8434382194934766, 0.778880226789511)\n",
      "Test Stats Epoch: (0.7633136094674556, 0.7888386123680241, 0.8171875, 0.7623906705539358)\n",
      "t=10.07474684715271\n",
      "\n",
      "Training Stats Epoch: (0.989369412901667, 0.9905151972407847, 0.9890228153250108, 0.9920120898100173)\n",
      "Val Stats Epoch: (0.7843551797040169, 0.8130498533724341, 0.8511128165771297, 0.7782456140350877)\n",
      "Test Stats Epoch: (0.7641589180050719, 0.7900677200902936, 0.8203125, 0.7619738751814223)\n",
      "t=13.722982168197632\n",
      "\n",
      "Training Stats Epoch: (0.9892486107755496, 0.9903981011975401, 0.9879466207490314, 0.9928617780661908)\n",
      "Val Stats Epoch: (0.7847780126849895, 0.8102869921729408, 0.8342287029930928, 0.7876811594202898)\n",
      "Test Stats Epoch: (0.7709213863060017, 0.7945413191811979, 0.81875, 0.7717231222385862)\n",
      "t=10.344084024429321\n",
      "\n",
      "Training Stats Epoch: (0.9886446001449626, 0.9898488120950325, 0.9864399483426604, 0.9932813177286519)\n",
      "Val Stats Epoch: (0.7847780126849895, 0.811829944547135, 0.8426707597851113, 0.7831669044222539)\n",
      "Test Stats Epoch: (0.7709213863060017, 0.7948523845571537, 0.8203125, 0.7709251101321586)\n",
      "t=13.700185060501099\n",
      "\n",
      "Training Stats Epoch: (0.987919787388258, 0.989247311827957, 0.9900990099009901, 0.9883970777825526)\n",
      "Val Stats Epoch: (0.7843551797040169, 0.8127753303964758, 0.8495778971603991, 0.7790288529204785)\n",
      "Test Stats Epoch: (0.7751479289940828, 0.8003003003003002, 0.8328125, 0.7702312138728323)\n",
      "t=13.888621807098389\n",
      "\n",
      "Training Stats Epoch: (0.9880405895143755, 0.9893399375471088, 0.9888075764098149, 0.9898728722258134)\n",
      "Val Stats Epoch: (0.7801268498942917, 0.8092443140132061, 0.8465080583269379, 0.77512297962052)\n",
      "Test Stats Epoch: (0.7743026204564666, 0.7993989481592787, 0.83125, 0.7698986975397974)\n",
      "t=13.76310420036316\n",
      "\n",
      "Training Stats Epoch: (0.987074172505436, 0.9884461721196415, 0.9851485148514851, 0.9917659804983748)\n",
      "Val Stats Epoch: (0.7843551797040169, 0.8137326515704895, 0.8549501151189562, 0.7763066202090593)\n",
      "Test Stats Epoch: (0.7768385460693153, 0.8026905829596412, 0.8390625, 0.7693409742120344)\n",
      "t=13.928431987762451\n",
      "\n",
      "Training Stats Epoch: (0.9881613916404929, 0.9894031141868512, 0.9847180370210934, 0.9941329856584094)\n",
      "Val Stats Epoch: (0.7771670190274842, 0.8023997000374953, 0.8211818879508825, 0.7844574780058651)\n",
      "Test Stats Epoch: (0.7616229923922232, 0.7853881278538813, 0.80625, 0.7655786350148368)\n",
      "t=10.184936285018921\n",
      "\n",
      "Training Stats Epoch: (0.9921478618023677, 0.9929934245984693, 0.9913904433921653, 0.9946015979270136)\n",
      "Val Stats Epoch: (0.7792811839323467, 0.8085106382978724, 0.8457405986185725, 0.7744202389318342)\n",
      "Test Stats Epoch: (0.7743026204564666, 0.8014869888475836, 0.8421875, 0.7645390070921986)\n",
      "t=9.976262092590332\n",
      "\n",
      "Training Stats Epoch: (0.9920270596762503, 0.9928986442866365, 0.9931123547137323, 0.9926850258175559)\n",
      "Val Stats Epoch: (0.7813953488372093, 0.812204867417363, 0.8580199539524175, 0.7710344827586207)\n",
      "Test Stats Epoch: (0.7666948436179205, 0.7958579881656805, 0.840625, 0.7556179775280899)\n",
      "t=13.800342082977295\n",
      "\n",
      "Training Stats Epoch: (0.990215027784489, 0.9912517550491413, 0.9877313818338356, 0.9947973119445047)\n",
      "Val Stats Epoch: (0.7830866807610993, 0.8102108768035517, 0.8403683806600154, 0.7821428571428571)\n",
      "Test Stats Epoch: (0.7709213863060017, 0.7948523845571537, 0.8203125, 0.7709251101321586)\n",
      "t=10.325472354888916\n",
      "\n",
      "Training Stats Epoch: (0.9900942256583716, 0.9911485319516408, 0.9881618596642273, 0.9941533131225638)\n",
      "Val Stats Epoch: (0.7906976744186046, 0.8178137651821862, 0.8526477359938603, 0.7857142857142857)\n",
      "Test Stats Epoch: (0.7717666948436179, 0.7972972972972974, 0.8296875, 0.7673410404624278)\n",
      "t=14.099522590637207\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10000\n",
    "for epoch in range(EPOCHS):\n",
    "    train(epoch, epoch % 100 == 0, epoch % 5 == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27ef3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# longformer graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f86d4438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8249577192558589,\n",
       " 0.8502325581395349,\n",
       " 0.8852776582006027,\n",
       " 0.8178564326903957)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_model(model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1b43eb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8038054968287527,\n",
       " 0.8292862398822662,\n",
       " 0.8649270913277053,\n",
       " 0.7964664310954064)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_model(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba474564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8157227387996618, 0.835843373493976, 0.8671875, 0.8066860465116279)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_model(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
