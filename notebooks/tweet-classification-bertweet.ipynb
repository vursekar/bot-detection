{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20849,"status":"ok","timestamp":1670387785770,"user":{"displayName":"Varun Ursekar","userId":"14716336135343578055"},"user_tz":300},"id":"a-GlywkSFegL","outputId":"6666b3a1-6c0d-440a-ec4e-5c60eff1fb0e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n","\u001b[K     |████████████████████████████████| 5.8 MB 30.6 MB/s \n","\u001b[?25hCollecting datasets\n","  Downloading datasets-2.7.1-py3-none-any.whl (451 kB)\n","\u001b[K     |████████████████████████████████| 451 kB 65.9 MB/s \n","\u001b[?25hCollecting evaluate\n","  Downloading evaluate-0.3.0-py3-none-any.whl (72 kB)\n","\u001b[K     |████████████████████████████████| 72 kB 643 kB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n","Collecting huggingface-hub<1.0,>=0.10.0\n","  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n","\u001b[K     |████████████████████████████████| 182 kB 37.7 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[K     |████████████████████████████████| 7.6 MB 55.0 MB/s \n","\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.8.3)\n","Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (9.0.0)\n","Collecting xxhash\n","  Downloading xxhash-3.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[K     |████████████████████████████████| 212 kB 28.8 MB/s \n","\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (2022.11.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (1.3.5)\n","Requirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.3.6)\n","Collecting responses<0.19\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Collecting multiprocess\n","  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n","\u001b[K     |████████████████████████████████| 132 kB 60.4 MB/s \n","\u001b[?25hRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.8.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (22.1.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.3)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (6.0.2)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.2)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (2.1.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.9.24)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n","Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 30.1 MB/s \n","\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2022.6)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n","Installing collected packages: urllib3, xxhash, responses, multiprocess, huggingface-hub, tokenizers, datasets, transformers, evaluate\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","Successfully installed datasets-2.7.1 evaluate-0.3.0 huggingface-hub-0.11.1 multiprocess-0.70.14 responses-0.18.0 tokenizers-0.13.2 transformers-4.25.1 urllib3-1.25.11 xxhash-3.1.0\n"]}],"source":["!pip install transformers datasets evaluate;"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26156,"status":"ok","timestamp":1670387811922,"user":{"displayName":"Varun Ursekar","userId":"14716336135343578055"},"user_tz":300},"id":"xbRo_DFQzar4","outputId":"efc40282-b4fe-4435-ea2a-5cc64a4b0caf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GbgCrjN5zZmY"},"outputs":[],"source":["import json\n","import os\n","import logging\n","import pandas as pd\n","\n","import torch\n","import torch.nn as nn\n","import evaluate\n","\n","from datasets import load_dataset, Dataset, DatasetDict\n","from transformers import AutoTokenizer, DataCollatorWithPadding, AutoModelForSequenceClassification, TrainingArguments, Trainer\n","\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dbeNXina9iVX"},"outputs":[],"source":["def get_dataset(path_to_data):\n","  with open(path_to_data) as f:\n","    data = json.loads(f.read())\n","  data = [[' </s> '.join(x['tweets']), 1 if x['label']=='bot' else 0] for x in data]\n","  df = pd.DataFrame(data)\n","  df.columns = [\"text\", \"label\"]\n","  data = Dataset.from_pandas(df)\n","  return data"]},{"cell_type":"code","source":["data_args = {'path_to_train' : '/content/drive/MyDrive/advNLP/twibot20/train.json',\n","             'path_to_test' : '/content/drive/MyDrive/advNLP/twibot20/test.json',\n","             'path_to_val' : '/content/drive/MyDrive/advNLP/twibot20/val.json'}\n","\n","max_length = 128\n","checkpoint = \"vinai/bertweet-base\""],"metadata":{"id":"6zl5tpuJWSib"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data = get_dataset(data_args['path_to_train'])\n","val_data = get_dataset(data_args['path_to_val'])\n","test_data = get_dataset(data_args['path_to_test'])\n","\n","data = DatasetDict({\n","    'train': train_data,\n","    'test': test_data,\n","    'valid': val_data})"],"metadata":{"id":"sX57O_aHWNk-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained(checkpoint)"],"metadata":{"id":"xu57xFRbWxBL","executionInfo":{"status":"ok","timestamp":1670390546599,"user_tz":300,"elapsed":2439,"user":{"displayName":"Varun Ursekar","userId":"14716336135343578055"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"4195c23d-d564-48ed-eadc-79fff9162502"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Could not locate the tokenizer configuration file, will try to use the model config instead.\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.25.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading file vocab.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/vocab.json\n","loading file merges.txt from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/merges.txt\n","loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/tokenizer.json\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at None\n","loading file tokenizer_config.json from cache at None\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.25.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n"]}]},{"cell_type":"code","source":["def preprocess_function(examples):\n","  return tokenizer(examples[\"text\"], truncation=True, padding=True, max_length=max_length)\n","\n","tokenized_dataset = data.map(preprocess_function, batched=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":133,"referenced_widgets":["2ac19d57686d4623a2e6dfde9dce29b7","b1ac3b6cf7da436cb8bb92d8bbaf0815","bfd53b1cf2ef42639f5ed9fef3319fb6","5975dfbdda7942a88d56fe2c3f9637a8","557a81de5d04469db3e10b0ae0b785fd","dd8d4453ab9848e2a8d804c82556e396","40d1431f86bb490283682f641eaabb15","51b8a19aede0413dbb6a9fa359087a6d","fb518d0ca60742829df05c26f80dc7e8","ad293b9a46ec49ca98476ad63bfb7dd3","2cc535d8a99e411bab5fcede3a696632","b96973fad94b45448e2c6482c1c7d16e","12984f4510c444dda408df8e4d433ab5","bbe9c746cd7e43bba00a74ead1759a43","5057d7a852b0447eafdf990d573bfe4b","fcb71f34297c40038d9717fc7dd96999","c4f4c4dd9d5446cba617e4f180054b1c","17ef7423cd94401c9c97aacf54b72836","8a0b3de36414441cb3f2599f086cae07","e72f631c379b4410afa8c9ff08ae1e37","cfc5159638b94391801053ebf6221a19","af8a4b5467e34f7ca85903cb5533adc6","539646060f1f44cba6db2975bf616771","bc73349b89574a2aa42ddbacccee77a4","4f7f4b071716479e9051d364ae1384c7","3583acd6cb2f4ebcb6453cf484f15972","c647b6dbb5fa457f97b070d8aa87a1ba","ef9b1a38aace4c64958c12368300beeb","1001f5dd92944099894acac2bd8de929","c48439393ac74a74af46e7b02d7d6abc","166904548a2e4b1094aa9651b8c4bb29","4f24605d5fd744dbad214db914abadf2","2b000d08e1974eadb18bef10edb320c3"]},"id":"H0qm6thHXCEr","executionInfo":{"status":"ok","timestamp":1670390701019,"user_tz":300,"elapsed":136584,"user":{"displayName":"Varun Ursekar","userId":"14716336135343578055"}},"outputId":"fc2642f4-abad-4743-c961-58761c74f521"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/9 [00:00<?, ?ba/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ac19d57686d4623a2e6dfde9dce29b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/2 [00:00<?, ?ba/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b96973fad94b45448e2c6482c1c7d16e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/3 [00:00<?, ?ba/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"539646060f1f44cba6db2975bf616771"}},"metadata":{}}]},{"cell_type":"code","source":["import numpy as np\n","\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","accuracy = evaluate.load(\"accuracy\")\n","\n","def compute_metrics(eval_pred):\n","  predictions, labels = eval_pred\n","  predictions = np.argmax(predictions, axis=1)\n","  return accuracy.compute(predictions=predictions, references=labels)"],"metadata":{"id":"D8qRP0WGXi0L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["id2label = {0: 0, 1: 1}\n","label2id = {0: 0, 1: 1}"],"metadata":{"id":"xL6ZF5uvZA3b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = AutoModelForSequenceClassification.from_pretrained(checkpoint, \n","                                                           num_labels=2, \n","                                                           id2label=id2label, \n","                                                           label2id=label2id)\n","\n","\n","freeze_pretrained_layers = True\n","\n","if freeze_pretrained_layers:\n","  for param in model.base_model.parameters():\n","      param.requires_grad = False"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N1pWSRUgZH4g","executionInfo":{"status":"ok","timestamp":1670390721279,"user_tz":300,"elapsed":3281,"user":{"displayName":"Varun Ursekar","userId":"14716336135343578055"}},"outputId":"d5a346c8-d306-4126-c8bc-2466359d5836"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": 0,\n","    \"1\": 1\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"0\": 0,\n","    \"1\": 1\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.25.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/pytorch_model.bin\n","Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["training_args = TrainingArguments(\n","    output_dir=\"/content/drive/MyDrive/advNLP/outputs/tweets/bertweet\",\n","    learning_rate=4e-5,\n","    per_device_train_batch_size=8,\n","    per_device_eval_batch_size=8,\n","    num_train_epochs=10,\n","    weight_decay=0.01,\n","    evaluation_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    load_best_model_at_end=True,\n","    push_to_hub=False,\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_dataset[\"train\"],\n","    eval_dataset=tokenized_dataset[\"valid\"],\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lxhQ-2qeZRQi","outputId":"595dc529-dd50-4f09-efc2-680fe4598ce7","executionInfo":{"status":"ok","timestamp":1670390732812,"user_tz":300,"elapsed":338,"user":{"displayName":"Varun Ursekar","userId":"14716336135343578055"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"]}]},{"cell_type":"code","source":["trainer.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"G_QsLycJ6pHD","outputId":"cc33050b-bb4b-4acc-9aa6-78a8d6520e18"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 8278\n","  Num Epochs = 10\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 10350\n","  Number of trainable parameters = 592130\n","You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='7067' max='10350' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 7067/10350 18:38 < 08:39, 6.31 it/s, Epoch 6.83/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.672100</td>\n","      <td>0.663927</td>\n","      <td>0.611416</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.656400</td>\n","      <td>0.646379</td>\n","      <td>0.635518</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.644800</td>\n","      <td>0.634326</td>\n","      <td>0.629175</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.633700</td>\n","      <td>0.620039</td>\n","      <td>0.668076</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.626600</td>\n","      <td>0.612360</td>\n","      <td>0.678647</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.624800</td>\n","      <td>0.608490</td>\n","      <td>0.682452</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 2365\n","  Batch size = 8\n","Saving model checkpoint to /content/drive/MyDrive/advNLP/outputs/tweets/roberta/checkpoint-1035\n","Configuration saved in /content/drive/MyDrive/advNLP/outputs/tweets/roberta/checkpoint-1035/config.json\n","Model weights saved in /content/drive/MyDrive/advNLP/outputs/tweets/roberta/checkpoint-1035/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/advNLP/outputs/tweets/roberta/checkpoint-1035/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/advNLP/outputs/tweets/roberta/checkpoint-1035/special_tokens_map.json\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 2365\n","  Batch size = 8\n","Saving model checkpoint to /content/drive/MyDrive/advNLP/outputs/tweets/roberta/checkpoint-2070\n","Configuration saved in /content/drive/MyDrive/advNLP/outputs/tweets/roberta/checkpoint-2070/config.json\n","Model weights saved in /content/drive/MyDrive/advNLP/outputs/tweets/roberta/checkpoint-2070/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/advNLP/outputs/tweets/roberta/checkpoint-2070/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/advNLP/outputs/tweets/roberta/checkpoint-2070/special_tokens_map.json\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 2365\n","  Batch size = 8\n","Saving model checkpoint to /content/drive/MyDrive/advNLP/outputs/tweets/roberta/checkpoint-3105\n","Configuration saved in /content/drive/MyDrive/advNLP/outputs/tweets/roberta/checkpoint-3105/config.json\n","Model weights saved in /content/drive/MyDrive/advNLP/outputs/tweets/roberta/checkpoint-3105/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/advNLP/outputs/tweets/roberta/checkpoint-3105/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/advNLP/outputs/tweets/roberta/checkpoint-3105/special_tokens_map.json\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 2365\n","  Batch size = 8\n","Saving model checkpoint to /content/drive/MyDrive/advNLP/outputs/tweets/roberta/checkpoint-4140\n","Configuration saved in /content/drive/MyDrive/advNLP/outputs/tweets/roberta/checkpoint-4140/config.json\n","Model weights saved in /content/drive/MyDrive/advNLP/outputs/tweets/roberta/checkpoint-4140/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/advNLP/outputs/tweets/roberta/checkpoint-4140/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/advNLP/outputs/tweets/roberta/checkpoint-4140/special_tokens_map.json\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 2365\n","  Batch size = 8\n","Saving model checkpoint to /content/drive/MyDrive/advNLP/outputs/tweets/roberta/checkpoint-5175\n","Configuration saved in /content/drive/MyDrive/advNLP/outputs/tweets/roberta/checkpoint-5175/config.json\n","Model weights saved in /content/drive/MyDrive/advNLP/outputs/tweets/roberta/checkpoint-5175/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/advNLP/outputs/tweets/roberta/checkpoint-5175/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/advNLP/outputs/tweets/roberta/checkpoint-5175/special_tokens_map.json\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 2365\n","  Batch size = 8\n","Saving model checkpoint to /content/drive/MyDrive/advNLP/outputs/tweets/roberta/checkpoint-6210\n","Configuration saved in /content/drive/MyDrive/advNLP/outputs/tweets/roberta/checkpoint-6210/config.json\n","Model weights saved in /content/drive/MyDrive/advNLP/outputs/tweets/roberta/checkpoint-6210/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/advNLP/outputs/tweets/roberta/checkpoint-6210/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/advNLP/outputs/tweets/roberta/checkpoint-6210/special_tokens_map.json\n"]}]},{"cell_type":"code","source":["from evaluate import evaluator\n","\n","task_evaluator = evaluator(\"text-classification\")\n","\n","test_results = task_evaluator.compute(\n","    model_or_pipeline=model,\n","    tokenizer=tokenizer,\n","    data=tokenized_dataset['test'],\n","    metric=evaluate.combine([\"accuracy\", \"recall\", \"precision\", \"f1\"]),\n","    label_mapping={0: 0, 1: 1}\n",")"],"metadata":{"id":"5MN_5eKDZeM8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670389915460,"user_tz":300,"elapsed":70647,"user":{"displayName":"Varun Ursekar","userId":"14716336135343578055"}},"outputId":"a503b65f-9d41-42de-cd48-0cf5ea51ff81"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Could not locate the tokenizer configuration file, will try to use the model config instead.\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.25.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading file vocab.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/vocab.json\n","loading file merges.txt from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/merges.txt\n","loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/tokenizer.json\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at None\n","loading file tokenizer_config.json from cache at None\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.25.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n"]}]},{"cell_type":"code","source":["test_results"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2UKfoL74_Nuh","executionInfo":{"status":"ok","timestamp":1670389915461,"user_tz":300,"elapsed":7,"user":{"displayName":"Varun Ursekar","userId":"14716336135343578055"}},"outputId":"12c98faf-34bb-4138-ae35-6acd7db98b3b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'accuracy': 0.6542688081149619,\n"," 'recall': 0.859375,\n"," 'precision': 0.6329113924050633,\n"," 'f1': 0.728959575878065,\n"," 'total_time_in_seconds': 62.13790452600006,\n"," 'samples_per_second': 19.038298909886848,\n"," 'latency_in_seconds': 0.05252570120541002}"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["eval_results = task_evaluator.compute(\n","    model_or_pipeline=model,\n","    tokenizer=tokenizer,\n","    data=tokenized_dataset['valid'],\n","    metric=evaluate.combine([\"accuracy\", \"recall\", \"precision\", \"f1\"]),\n","    label_mapping={0: 0, 1: 1}\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4hh1CkA6_uFA","executionInfo":{"status":"ok","timestamp":1670390255751,"user_tz":300,"elapsed":117672,"user":{"displayName":"Varun Ursekar","userId":"14716336135343578055"}},"outputId":"6a1263dc-c585-4847-f674-e4813a4364f4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Could not locate the tokenizer configuration file, will try to use the model config instead.\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.25.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading file vocab.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/vocab.json\n","loading file merges.txt from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/merges.txt\n","loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/tokenizer.json\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at None\n","loading file tokenizer_config.json from cache at None\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.25.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n"]}]},{"cell_type":"code","source":["eval_results"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"App6xqPxAwJk","executionInfo":{"status":"ok","timestamp":1670390405670,"user_tz":300,"elapsed":341,"user":{"displayName":"Varun Ursekar","userId":"14716336135343578055"}},"outputId":"b0e50210-442b-4407-90f5-67461777dc11"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'accuracy': 0.678646934460888,\n"," 'recall': 0.8894858019953953,\n"," 'precision': 0.6529577464788733,\n"," 'f1': 0.7530864197530863,\n"," 'total_time_in_seconds': 109.66953663699996,\n"," 'samples_per_second': 21.564785194889787,\n"," 'latency_in_seconds': 0.04637189709809724}"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":[],"metadata":{"id":"pX1IxLOEAsEQ"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"https://github.com/DhavalTaunk08/NLP_scripts/blob/master/sentiment_analysis_using_roberta.ipynb","timestamp":1670168455148}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"},"widgets":{"application/vnd.jupyter.widget-state+json":{"2ac19d57686d4623a2e6dfde9dce29b7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b1ac3b6cf7da436cb8bb92d8bbaf0815","IPY_MODEL_bfd53b1cf2ef42639f5ed9fef3319fb6","IPY_MODEL_5975dfbdda7942a88d56fe2c3f9637a8"],"layout":"IPY_MODEL_557a81de5d04469db3e10b0ae0b785fd"}},"b1ac3b6cf7da436cb8bb92d8bbaf0815":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dd8d4453ab9848e2a8d804c82556e396","placeholder":"​","style":"IPY_MODEL_40d1431f86bb490283682f641eaabb15","value":"100%"}},"bfd53b1cf2ef42639f5ed9fef3319fb6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_51b8a19aede0413dbb6a9fa359087a6d","max":9,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fb518d0ca60742829df05c26f80dc7e8","value":9}},"5975dfbdda7942a88d56fe2c3f9637a8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ad293b9a46ec49ca98476ad63bfb7dd3","placeholder":"​","style":"IPY_MODEL_2cc535d8a99e411bab5fcede3a696632","value":" 9/9 [01:34&lt;00:00,  9.20s/ba]"}},"557a81de5d04469db3e10b0ae0b785fd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dd8d4453ab9848e2a8d804c82556e396":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"40d1431f86bb490283682f641eaabb15":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"51b8a19aede0413dbb6a9fa359087a6d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fb518d0ca60742829df05c26f80dc7e8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ad293b9a46ec49ca98476ad63bfb7dd3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2cc535d8a99e411bab5fcede3a696632":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b96973fad94b45448e2c6482c1c7d16e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_12984f4510c444dda408df8e4d433ab5","IPY_MODEL_bbe9c746cd7e43bba00a74ead1759a43","IPY_MODEL_5057d7a852b0447eafdf990d573bfe4b"],"layout":"IPY_MODEL_fcb71f34297c40038d9717fc7dd96999"}},"12984f4510c444dda408df8e4d433ab5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c4f4c4dd9d5446cba617e4f180054b1c","placeholder":"​","style":"IPY_MODEL_17ef7423cd94401c9c97aacf54b72836","value":"100%"}},"bbe9c746cd7e43bba00a74ead1759a43":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8a0b3de36414441cb3f2599f086cae07","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e72f631c379b4410afa8c9ff08ae1e37","value":2}},"5057d7a852b0447eafdf990d573bfe4b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cfc5159638b94391801053ebf6221a19","placeholder":"​","style":"IPY_MODEL_af8a4b5467e34f7ca85903cb5533adc6","value":" 2/2 [00:13&lt;00:00,  5.96s/ba]"}},"fcb71f34297c40038d9717fc7dd96999":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c4f4c4dd9d5446cba617e4f180054b1c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"17ef7423cd94401c9c97aacf54b72836":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8a0b3de36414441cb3f2599f086cae07":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e72f631c379b4410afa8c9ff08ae1e37":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cfc5159638b94391801053ebf6221a19":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"af8a4b5467e34f7ca85903cb5533adc6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"539646060f1f44cba6db2975bf616771":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bc73349b89574a2aa42ddbacccee77a4","IPY_MODEL_4f7f4b071716479e9051d364ae1384c7","IPY_MODEL_3583acd6cb2f4ebcb6453cf484f15972"],"layout":"IPY_MODEL_c647b6dbb5fa457f97b070d8aa87a1ba"}},"bc73349b89574a2aa42ddbacccee77a4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ef9b1a38aace4c64958c12368300beeb","placeholder":"​","style":"IPY_MODEL_1001f5dd92944099894acac2bd8de929","value":"100%"}},"4f7f4b071716479e9051d364ae1384c7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c48439393ac74a74af46e7b02d7d6abc","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_166904548a2e4b1094aa9651b8c4bb29","value":3}},"3583acd6cb2f4ebcb6453cf484f15972":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4f24605d5fd744dbad214db914abadf2","placeholder":"​","style":"IPY_MODEL_2b000d08e1974eadb18bef10edb320c3","value":" 3/3 [00:27&lt;00:00,  8.31s/ba]"}},"c647b6dbb5fa457f97b070d8aa87a1ba":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef9b1a38aace4c64958c12368300beeb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1001f5dd92944099894acac2bd8de929":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c48439393ac74a74af46e7b02d7d6abc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"166904548a2e4b1094aa9651b8c4bb29":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4f24605d5fd744dbad214db914abadf2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2b000d08e1974eadb18bef10edb320c3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}